{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **INTRO**\n",
        "\n",
        "The dataset which I have chosen is “League of Legends Diamond Ranked Games (10 min)”. The dataset contains a lot of relevant information at the 10 minute mark of a game of League of Legends and which team won the game. Diamond is the rank of the game, meaning that all of the people in the match are good players and are competent. A game of League of Legends will last thirty or forty minutes. We can use the statistics available to us in the first 10 minutes of the game to see who will end up winning the match.\n",
        "\n",
        "This was interesting for me specifically as a long time high-elo player of League of Legends. I wanted to see what good players do to win their games and how I can follow in their footsteps and be more likely to win my games in the long term.\n",
        "\n",
        "Let me give an explanation of the game. League of Legends is a 5 versus 5 “multi online battle arena”. The 10 players are put into two teams of five - Red and Blue. Each team has a base called a nexus. The other team wins if they destroy the team’s nexus. The nexus is protected by multiple towers which must be beaten first before destroying the enemy team’s nexus. There are also “minions” which are small AIs which appear for both teams and automatically go to the nearest tower to destroy it. They are a source of income for the players. Players in league of legends can buy “items” with “gold”. Items make the character strong. Without items each character is weak. Getting gold involves killing the enemy minions / players / towers. There is also something called vision. Players can buy “wards” which are simply lamps that are used to grant vision of the map otherwise they will not see what is there. Lastly there are “dragons” which give the players big buffs if they kill them.\n",
        "\n",
        "This is all interesting as we can see how a team is doing at the beginning of the long game by checking these metrics and then estimate what the outcome of the game will be based on which team has the most towers, gold and so forth in these early moments. I believe this data can be used in a professional analysis to help with people who are trying to become better at the video game or are playing for a professional league. This model will help tweak the plans of professional teams as they go into e-sports.\n",
        "\n",
        "#**Visualisation**\n",
        "\n",
        "In this section I used many visualisation methods to explore my data. The histograms showed me that the dataset is pretty balanced. We could see that 50% of the time the blue side wins and 50% of the time blue loses. This is the same with all binaries. First blood is split evenely between the teams too. The scatter plot showed me that the more kills blue has the less deaths they will have. Furthermore blue kills and average level seem to be linked. Using SNS pairplot we can see that the more kills then the more likely blue is to win. The correlation matrix showed me that the three most correlated features to wins is blue kills, blue total gold and blue assists. Blue heralds had very little correlation. More kills means more gold. More deaths means more likely to lose. With this all being said - the correlation between kills and wins is only 0.34 so that tells me that kills in the first 10 minutes of the game are not extremely significant.\n",
        "\n",
        "#**Preprocessing**\n",
        "\n",
        "For preprocessing I made many changes to make my data more useable. I introduced new features like KDDiff, CSDiff and visionScore. These are three separate attributes which are useful to tell how good a team is playing. KDDiff is difference in kills and deaths. CSDIff is how good the team is at getting non player enemies which will give gold. Vision Score is how good the team is at making dark areas of the map visible. I also dropped many features which were not necessary. GameID has no use. Many were the red features as they are usually the opposite of the blue team. I then used a scalar to scale my attributes. This is because there can be big differences in the attributes. Some attributes can be 0-10 and some can be 0-10000. Outliers were removed by calculating the mean for every feature and removing any record which had records 3 standard deviation away. Finally I checked for empty values and dropped.\n",
        "\n",
        "#**Metrics**\n",
        "\n",
        "I created three functions which I would use to measure the use of my models. Print metrics will show the TP, FP, TN, FN, Accuracy and ROC. This can be used on any model. After using this with naive bayes I can see that  the initial dataset has a precision of 0.71 and a ROC score of 0.792944. This gives me the impression that 10 minutes of the game is not enough data to be fully accurate when deciding who wins or loses the game.\n",
        "\n",
        "#**Feature Selection**\n",
        "\n",
        "I have used 6 different feature selection methods to try and get the best attributes from my dataset. The first is using variance threshold to reduce columns which have many repeating values. This had no impact on precision and had a ROC score of 0.207. I used the f_class algorithm which had a negative impact on precision and ROC. Next, random forest also yielded very similar results. I also tried to pick features myself based on my data visualisation and found that it increased ROC to 0.226 Linear SVC increased ROC score and recall for 2 * 2 attributes making me think this was the best algorithm. The trees classifier yielded similar results. I believe this is because of the naive classifier not taking into account that the features are correlated to each other. I found that the best feature selection method here was using the coorelation matrix as it gave the highest area ROC.\n",
        "\n",
        "#**Conclusion**\n",
        "\n",
        "I split my dataset into three datasets with 4, 10 and 30 columns each respectively (plus blueWins). I found that the dataset with 4 attributes was the best as it had a slightly better accuracy, recall and AUROC. I have realized however that this is not a massive difference from the original dataset. The conclusion I have come to is that 10 minutes is a good enough amount of data to predict what team will win with a 70-75% accuracy. This is not terrible but there is still many false positives and too false negatives - but this has a different conclusion that says a game is not over only based on the first minutes. This is good knowledge to know as many people in League say the game is lost too early and decide to leave the game. This data can be used to show us that actually a game is more flexible then you think. But if we really want to be fully precise then we should have more then only 10 minutes of the game.\n"
      ],
      "metadata": {
        "id": "p9BJkJSqe_Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------"
      ],
      "metadata": {
        "id": "8_S7sqORfORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CLUSTERING**\n",
        "\n",
        "#**Plotting**\n",
        "I created a plotting method to plot my clusters. I go through the cluster labels which have all the points. These are then plotted with the viridis color scheme based on the cluster they are member of.\n",
        "I used the elbow method to find the best number of clusters for k-means clustering. I found that using the elbow method the best number of clusters should be 4 clusters.\n",
        "I used a bandwidth estimation algorithm to get the bandwidth for my meanshift clustering.\n",
        "\n",
        "#**Statistics**\n",
        "As for how I analyzed my clusters I used the following metrics.\n",
        "\n",
        "**Using true and guess labels.**\n",
        "\n",
        "Fowlkes Mallows:\n",
        "The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall. Score ranges from 0 to 1. The higher the better.\n",
        "\n",
        "Mutual Information:\n",
        "Mutual Information measures the agreement of the labelling, ignoring permutations. From 0 to 1. The higher the better.\n",
        "\n",
        "Homogenity:\n",
        "Each cluster contains only members of a single class.\n",
        "\n",
        "Completeness:\n",
        "All members of a given class are assigned to the same cluster.\n",
        "\n",
        "**True label-less**\n",
        "\n",
        "Calinski-Harabasz:\n",
        "Ratio of the sum of between-clusters dispersion and of within-cluster dispersion.\n",
        "\n",
        "Silhouette:\n",
        "The mean of a sample from all the points in its cluster and in the next cluster.\n",
        "\n",
        "#**K-Means**\n",
        "I used K Means clustering. K means clustering tries to reduce the within cluster sum of squares and seperate the data into a bunch of specified number of clusters. By subplotting the data we can see that the data is seperated into equal clusters and we can clearly see the differences between them.\n",
        "We can see that using our different metrics that 2 clusters is the highest accuracy as according to fowlkes mallows and the rand score. We can see that K Means is not homogeonus and is not very high with completeness. It has a high Harabasz score and a low silhouette score.\n",
        "\n",
        "#**Mean-Shift**\n",
        "This method tries to find blobs around centers. Plotting the gold difference / cs per min we found only 1 blob. We found the same with blue gold dif and vision score.  We found that this technique gives a high fowlkes mallows score which means a high similarity. The clustering is not homegenius at all but is complete. Has a low silhouette score. Mutual information is unclear.\n",
        "\n",
        "#**AP**\n",
        "This clustering sends messages from a sample to another until convergence. From our plots we can see that there is very little space between the clusters. From our analysis 50% damping seems to be the best damping for this clustering as it has the highest mallow score and is complete. 60% damping has a low mallows score (10%) but better homogenity. The 70 and 80 percent are also the same. Mutual information is bad (0.05).\n",
        "\n",
        "#**Spectral Clusters**\n",
        "Spectral Clustering performs a low-dimension embedding of the affinity matrix between samples. We can see through our Clusters plot that even though multiple clusters are identified one is always dominating of the others. Regardless of the number of clusters asked to be identified the rand score is always very bad but the fowlkes score is solid. The Mutual information is low and the homogenus and completee index is also bad.\n",
        "\n",
        "#**Bayes Comparison**\n",
        "We can see that in comparison to the bayes classificaiton clustering did in fact work worse on average. Bayes got us around about 73 percent accuracy while clustering in different techniques fluctuated between 50 and 70% and in some cases gave us very low scores. The k means clustering technique was the closest in performance while the spectral shift clustering was the furthest.\n",
        "\n",
        "#**Best Clustering Technique**\n",
        "K Means was the best clustering method as it had the best mutual score and a good fowlkes score. We found that the best cluster k was actually 2 clusters which went against the plot of the elbow~method done earlier. All other methods faltered in mutual information. Affinity Propagation was the second best technique but only when damping with 50 percent dampness. Mean shift showed that blobs were unlikely to form in our data. Spectral shifting had a good fowlkes measure but was bad in other forms.\n",
        "\n",
        "All the clusters had a 100 percent complete score except for spectral clusters which was not complete. The most homogenus cluster was affinity propagation with k means behind it. The other 2 were not homogenus.\n",
        "\n",
        "In terms of plots k means and AP gave us the most meaningful results as we could our data being divided and parted in k means and we could see the distribution of clustering in affinity propagation clearly.\n",
        "\n",
        "Overall the greatest clustering technique is k means but AP is also good."
      ],
      "metadata": {
        "id": "WpYyRFvsfAvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------"
      ],
      "metadata": {
        "id": "D3XfkarEfQLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***TREES***\n",
        "#-----------\n",
        "#**Decision Tree  Classifier using Training Set**\n",
        "When testing the decision tree using my training set with default settings I found an accuracy of 100 percent due to the model getting too used to the training data. I tested both GINI and ENTROPY and found that Gini gave a better accuracy but there was barley any difference. Best also gave better performance over random by a couple of points. I found that 3 is the best depth for the tree. I experimented and found that when increasing the depth to 10 the accuracy is 84 percent and 20 d makes the accuracy 100% this is because the data is being fitted to the model overtly. I found the best number of max features to be 3 because 2 features is not enough and more features makes little difference. The number of min leaf I chose is 5 because it helps be more selective and does not impace performance. A alpha of 0.001 helped improve the tree slightly. Bigger numbers resulted in a 50% gini score. I noticed that when moving 30% of the data to the test set this resulted in a lower accuracy then when moving 60% of the data to the test set which means that the larger the test set has some overfitting issues (0.60 vs 0.68).\n",
        "\n",
        "#**Ten Fold Cross Validation**\n",
        "Testing this model with the default gave an accuracy of 61 percent. We can improve this. I found that both gini and ENTROPY gave very similar scores. Interestingly when reducing themaxdepth the accuracy of cross validation increased. I found that the best split method was more accurate than the random. The best max depth is 2. For some reason the deeper the depth the worse the accuracy. The more features the better, we will pick 5 as it is within the reasonable range and gives better performance than the others. The number of leafs needed made very little difference and so we chose a high number with 6. We faced the same issue with the alpha values as before so we used 0.001 as our pruning. The number of folds was tested. All folds from 2 to 10 gave the same or very similar result. Actually, when splitting up the training set I saw that 0.3% in the test set gave a higher accuracy - 0.69 percent - when compared to 0.6 which gave 0.66 percent.\n",
        "\n",
        "#**Decision Tree Classifier using Training and Test**\n",
        "We can see a great change from the very beginning. Default tree settings gives us 0.63% accuracy as opposed to 100% so we can see there is no overfitting. Gini was once again the better crit. Best gave the best split. Interestingly the higher the depth the worse the model worked. This is unlike the previous classifier and we found that a max depth of 2 was good. For max features the more the better but to avoid overfit we used 2 max features. The min number of leaves made litte difference but once again to avoid overfitting we chose a high number of 5. The best ccp was 0.002 but we saw little difference. We saw that putting 30% of the data into the test set gave us worse results but 3 percent.\n",
        "\n",
        "#**Random Forest on Training**\n",
        "We can see the same behavour where with default settings we get 100% accuracy. We find that both GINI and Entropy criterion are exactly the same in terms of precision recall and accuracy. As for depth, the more the better the model is. The depths from 2-6 give very close results so we will use 2. The model is not affected by the max number of features so we will use 2 as our max. The same with min leaves so we will use 5. We found the ccp value to have very little impact on our tree. Finally splitting our data into thirty precent and 60 percent made very little difference aswell.\n",
        "\n",
        "#**Ten Fold RandomForest**\n",
        "This tree performs slightly better than its counterpart with a point of 0.6853% however we still do not get the full 100 percent that we see when not using cross validation. Gini and entropy perform with no difference. The max number of features makes no difference so we choose 2. There is a very small difference between min leaf so we choose 6. We ignore CCP there is no difference. There is a very small difference between 30% and 60% in our test set. The best number of folds is 6.\n",
        "\n",
        "#**Ten Fold Random TrainingTest**\n",
        "We get an accuracy of 70% which is better than the counterpart but not 100. Gini and Entropy perform the same. We will use 3 as our depth as there is very little difference between them. No difference between number of max features, we use two. Small difference between the min leaf number - use 4. Alpha is negligable so we ignore. No difference between the percentage we use in our testing data.\n",
        "\n",
        "#**Conclusion**\n",
        "I noticed overfitting mainly when the depth of the tree was not controlled. In both instances when using the training data only we found that our tree or random forest tree gave us 100% accuracy which means the data is being matched perfectly which is wrong. I tried to eliminate over fitting by choosing the lowest great number for the minumum parameters and the highest number for the maximum. The tree does generalize well to new data getting about 70% accuracy which is the same as our naive bayes model.\n"
      ],
      "metadata": {
        "id": "CWL6SzKkfDOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "zGB6MOwNfQ0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Neural***\n",
        "\n",
        "\n",
        "The score of the neural network is 70% at default stats.\n",
        "\n",
        "We did some testing and found that identity and logarithm are both the same in terms of performance for activation function (71%). Relu performed ok at 70% and Tanh performed fine at 68%. Thus we can see that the activation functions improved performance. The activation function is what decided if the neuron should start or not.\n",
        "\n",
        " The relu made performance worse. Relu gets rid of neurons if the weight are under 0 and thus we can say that for our model we needed some weights to be under 0. The tan function is bad too as it is good for very leaned value.\n",
        "\n",
        "As for the size we found that the best number and size is 20 and 30 but the difference was very little.\n",
        "The learning rate made no difference in the model.\n",
        "The best number of iterations was 40 increasing the accuracy by 15percent.\n",
        "\n",
        "The threshold made very little difference. We did validation on the classifier. Momentum made no difference.\n",
        "Overall the model is not extremely linearly seperable as it does not converge even after increasing the things that make it overfi. The model is better than a simple linear one though.\n"
      ],
      "metadata": {
        "id": "n-r6zmQEfHDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "Kun9degnfSAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***LINEAR REGRESSION***\n",
        "The use of SGD regression on the dataset gave us an accuracy of 68.8% without splitting the data into test. When the data was split into test the accuracy 63 percent.\n",
        "\n",
        "As for tenfold validation on it when not splitting any data into the test set the accuracy was 66%. After splitting the data into training and test a average of 60% was gotten.\n",
        "\n",
        "The linear classer does not perform badly on new data as it only had a approximate 5.5 percent drop even linear classifier did perform worse then the other models.\n",
        "\n",
        "The data is not linearly separable as convergence is not reached and it still has a relatively low accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "y2VgQKrXfI3V"
      }
    }
  ]
}